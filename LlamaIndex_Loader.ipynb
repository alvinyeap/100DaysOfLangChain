{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjsnO1z9/WCEZdWSxtELaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvinyeap/100DaysOfLangChain/blob/Day002/LlamaIndex_Loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome!\n",
        "This is from my #100DaysOfLangChain journey.\n",
        "\n",
        "If you like this, follow me @ [https://twitter.com/aigen__](https://twitter.com/aigen__)"
      ],
      "metadata": {
        "id": "iYAebKrXvwtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex Loaders\n",
        "Document loaders are the first step in getting new knowledge in to an LLM.\n",
        "\n",
        "Here we're using LlamaIndex's YoutubeTranscriptReader to load the transcript for James Brigg's excellent Youtube video on prepping data!"
      ],
      "metadata": {
        "id": "uuMqYeW6v8fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LlamaIndex (aka gpt_index)\n",
        "!pip install llama_index"
      ],
      "metadata": {
        "id": "-7OpRLdbvCvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import download_loader\n",
        "\n",
        "# Create the loader\n",
        "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\") \n",
        "loader = YoutubeTranscriptReader()\n",
        "\n",
        "# Load the transcript from Youtube!\n",
        "# We're going to load it in a LangChain format\n",
        "documents = loader.load_langchain_documents(ytlinks=['https://www.youtube.com/watch?v=eqOfr4AGLk8&t'])"
      ],
      "metadata": {
        "id": "XigPgPrvzj_c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out the content!\n",
        "print(documents[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcRssy-BznKT",
        "outputId": "0165a09f-fc45-4ca0-c16f-65673b24cfa6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in this video we are going to take a\n",
            "look at what we need to do and what we\n",
            "need to consider when we are chunking\n",
            "text for large language models the best\n",
            "way I can think of or demonstrating this\n",
            "is to walk through an example now we're\n",
            "going to really go with the what I\n",
            "believe is kind of like a rule of thumb\n",
            "that I tend to use when I'm when I'm\n",
            "chunking text in order to put into a\n",
            "large language model and it doesn't\n",
            "necessarily apply to every use case you\n",
            "know every use case is slightly\n",
            "different but I think this is a pretty\n",
            "good Approach at least when we're using\n",
            "retrieval augmentation and large\n",
            "language models which I think is where\n",
            "the chunking question kind of comes up\n",
            "most often so let's jump straight into\n",
            "it in this example what we're going to\n",
            "be doing is taking the Lang chain Dots\n",
            "here literally every page on this\n",
            "website and we're going to be\n",
            "downloading those taking each one of\n",
            "these pages and then we're going to\n",
            "splitting them into more reasonably\n",
            "sized chunks now how are we going to do\n",
            "this we're going to take a look at this\n",
            "notebook here now if you'd like to\n",
            "follow along with the code you can also\n",
            "run this notebook I will leave a link to\n",
            "it which will appear somewhere near the\n",
            "top of the video right now now to get\n",
            "started we're going to be using a few\n",
            "python libraries Lang chain is a pretty\n",
            "big one here so not only is it the\n",
            "documentation that we're downloading but\n",
            "it's also going to be\n",
            "how we download that documentation and\n",
            "it's also going to be how we split that\n",
            "documentation into Trunks and another\n",
            "dependency here is the tick token\n",
            "tokenizer we'll talk about that later\n",
            "and we're just going to visualize and\n",
            "make things a little bit easier to\n",
            "follow with these libraries here so in\n",
            "this example first thing we're going to\n",
            "do is download all of the dots from line\n",
            "chain so everything is contained within\n",
            "this is the top level page of the line\n",
            "train dubs we're going to save\n",
            "everything into this directory here and\n",
            "where we are going to say we want to get\n",
            "all of the dot HTML files okay\n",
            "so we run that and that will take a\n",
            "moment just to download everything that\n",
            "there's a lot in there my internet\n",
            "connection is also pretty slow so it\n",
            "will probably take me a moment but let's\n",
            "go ahead and just have a look at where\n",
            "these are being downloaded\n",
            "so if we come over to the left here we\n",
            "can see there is the RT dots repository\n",
            "there and inside the RT dots we have\n",
            "this line training with Doug's en latest\n",
            "so it's just kind of like a path of our\n",
            "dots and Okay cool so in there you can\n",
            "see everything's been downloaded we have\n",
            "like the index page which I think is the\n",
            "top level page obviously it's just it's\n",
            "HTML okay so it's kind of like when\n",
            "we're not going to process this we're\n",
            "going to use long chain to clean this up\n",
            "but if we come down a little bit I think\n",
            "maybe we can see something okay so this\n",
            "is like the the first page welcome to\n",
            "langjain l m is our immersion is a\n",
            "transformative technology so on and so\n",
            "on okay and we have some other things\n",
            "other pages\n",
            "yeah we're just going to process all\n",
            "this so back to our code uh it's done\n",
            "downloading now we can come down to here\n",
            "and what we're going to do is use the\n",
            "light chain document loaders and we're\n",
            "going to use the read the dots loader so\n",
            "read the dots is a specific template\n",
            "that is used quite often for\n",
            "documentation for code libraries\n",
            "and Lang chain includes a document\n",
            "loader that is specifically built for\n",
            "reading that type of documentation or\n",
            "that those HTML pages and processing\n",
            "them into a nicer format so it's really\n",
            "easy to use it we just point it to our\n",
            "directory that we just created and what\n",
            "are we doing here so we're loading those\n",
            "dots and here I'm just printing out the\n",
            "length of those dots so that we can see\n",
            "okay we have 390 HTML pages that have\n",
            "been downloaded there some reason okay\n",
            "so when I when I ran this about an hour\n",
            "ago they they actually had 389 now they\n",
            "have 390 pages so\n",
            "already updates\n",
            "cool all right let's have a look at one\n",
            "of those pages and so we have this we\n",
            "have this document object inside that we\n",
            "have page content which is all about\n",
            "tips all right if we want to print that\n",
            "in a nicer format we can see this okay\n",
            "all right it looks looks pretty good\n",
            "there's a lot of you know there is some\n",
            "kind of\n",
            "messy parts of this but it's not really\n",
            "a problem that the we could try and\n",
            "process that if we wanted to but\n",
            "honestly I don't really think it's worth\n",
            "it because the large orange model can\n",
            "handle this very easily\n",
            "so yeah I personally wouldn't really\n",
            "bother with that I'd just take it as it\n",
            "is now at the end of this object we come\n",
            "right to the end if it lets me\n",
            "we see that we have this metadata here\n",
            "okay inside the metadata we have the\n",
            "source which is in this case like the\n",
            "file path but fortunately the way that\n",
            "we've I set this up is that we can just\n",
            "replace RT dots with https and that will\n",
            "give us the URL for this particular file\n",
            "so let's come down here and you can see\n",
            "that's what I'm doing here replace RT\n",
            "duct with https\n",
            "cool and then we can click that and we\n",
            "come over to here now this is where we\n",
            "start talking about the chunking of what\n",
            "we're doing when we are thinking about\n",
            "chunking there are there are a few\n",
            "things to consider okay so the first\n",
            "thing to consider is how much text or\n",
            "how many tokens Can our large language\n",
            "model or whatever process is we're doing\n",
            "how many tokens Can it handle what is\n",
            "optimal for our particular use case the\n",
            "use case that I'm envisioning here is\n",
            "retrieval augmentation for like question\n",
            "answering using a larger language model\n",
            "so what does that mean exactly\n",
            "it's probably best if I draw it out so\n",
            "we're going to have our large language\n",
            "model over here and we're going to ask\n",
            "you a question so we have a question\n",
            "over here supposed to be a queue it's\n",
            "fine so we have our question like we're\n",
            "going to say\n",
            "uh what is the LM chain in Lang chain\n",
            "right\n",
            "if we pass that strain to our large\n",
            "language model at the moment using GPT\n",
            "3.5 turbo even GT4 they can't answer\n",
            "that question because they don't know\n",
            "what the line chain library is so in\n",
            "this scenario what we will do is we'd go\n",
            "to Vector database\n",
            "you know we don't really need to get in\n",
            "too much detail here we go to about the\n",
            "database which is where we store all of\n",
            "the documents I'm that we're processing\n",
            "now so all those line chain dots they\n",
            "would end up within that space and they\n",
            "would be retrieved and we would pass in\n",
            "like five or so of these chunks of text\n",
            "that are relevant to our particular\n",
            "query\n",
            "alongside our original query okay so\n",
            "what you'd end up with is rather than\n",
            "okay let's say this is your prompt you\n",
            "typically have your your query rather\n",
            "than just a query you'd have your query\n",
            "and then you'd also have these five like\n",
            "bits of relevant information below the\n",
            "query okay and that would all go into\n",
            "the large language model and you would\n",
            "essentially say to it you'd probably\n",
            "have some instructions and at the top\n",
            "and those instructions would say I want\n",
            "you to answer this question you'd maybe\n",
            "give the the question there to give it a\n",
            "bit later on using the context that we\n",
            "have provided and you would basically in\n",
            "front of each contacts you would write\n",
            "like context okay and the large launch\n",
            "model will answer the question based on\n",
            "those context right so that that's the\n",
            "scenario we're envisioning here and in\n",
            "this scenario if we want to input five\n",
            "of these contacts into each one of our\n",
            "retrieval augmented queries\n",
            "we need to think okay what is the max\n",
            "token limit of our large language model\n",
            "and how much of that space can be\n",
            "reserved for these contacts so in this\n",
            "scenario let's say that we're using gbt\n",
            "3.5 turbo the token limit for GPT 3.5\n",
            "turbo is something like 4 0 96.\n",
            "so this includes both all right so you\n",
            "have your large language model I'm gonna\n",
            "put that here this is standards to your\n",
            "large language model this 4096 includes\n",
            "the input\n",
            "to the large language model so all of\n",
            "your input tokens and also all of your\n",
            "generated output tokens\n",
            "okay and so basically we can't just use\n",
            "that for 4 000 tokens on the input we\n",
            "need to leave some space for the output\n",
            "and also within the input we have other\n",
            "components right so it's not just the\n",
            "context\n",
            "but we also have the query\n",
            "I mean that's supposed to say query and\n",
            "as well as that we might also have some\n",
            "instructions\n",
            "I don't know why am I right it's so bad\n",
            "and as well as the instructions might\n",
            "also have a bit of tracked history if\n",
            "this is a a chatbot okay so basically uh\n",
            "the amount of context that we can feed\n",
            "in is actually pretty Limited in this\n",
            "scenario let's just assume that we can\n",
            "we can pass in a context of around half\n",
            "of the 4 000 tokens so we'll say 2000 is\n",
            "going to be our limit okay if 2000 is\n",
            "our limit\n",
            "we that means we need to divide that by\n",
            "five because\n",
            "there's 2 000 tokens need to be shared\n",
            "by our five contacts\n",
            "which leaves us with about\n",
            "400 of these tokens per context okay so\n",
            "that's our maximum chunk size now one\n",
            "question that we might have here is\n",
            "could we reduce the number of tokens\n",
            "further and for sure we can okay so I\n",
            "would say the minimum number of tokens\n",
            "that you need within a context is for\n",
            "you to read this context\n",
            "does it make sense right if you have\n",
            "enough words in there for that context\n",
            "to make sense to you as a as a human\n",
            "being then that means that it is\n",
            "probably enough\n",
            "to feed as a chunk of text into a large\n",
            "language model into a betting model and\n",
            "so on so if that chunk of text has\n",
            "enough text in there to have some sort\n",
            "of meaning to itself then the chunk is\n",
            "probably big enough so as long as you\n",
            "satisfy that that should be the criteria\n",
            "for your minimum size of that chunk of\n",
            "text naturally for the maximum size of a\n",
            "jungle tapes we have the 400 tokens that\n",
            "we just calculated now so with that all\n",
            "of that in mind we need to take a look\n",
            "at how we would actually calculate the\n",
            "the size of these chunks okay because\n",
            "we're not basing us on character length\n",
            "we're based on this on tokenlab\n",
            "so in order to do that we need to look\n",
            "at how to tokenize text using the same\n",
            "tokenizer that our large language model\n",
            "users and then we can actually count\n",
            "number of tokens within each chunk\n",
            "so getting started with that we are\n",
            "going to be using the tick token\n",
            "tokenizer now this is specific to open\n",
            "AI models obviously if you're using\n",
            "cohere hug and face and so on this is\n",
            "going to be a slightly different\n",
            "approach so first we want to get our\n",
            "encoding so there are multiple tick\n",
            "token tokenizers that open our users\n",
            "this is just one of those now let's\n",
            "initialize that and I'll talk about a\n",
            "little bit about where we're getting\n",
            "these encoders from so you can actually\n",
            "find details for the tokenizer at this\n",
            "link here so this link is in their\n",
            "GitHub repo tick token tick\n",
            "tokenmodel.pi okay so I'm going to click\n",
            "through to that okay so this is in the\n",
            "opening hour tick token repository on\n",
            "GitHub and you can see we have this\n",
            "model to encoding a dictionary here and\n",
            "within this you can see that we have a\n",
            "mapping from each of the models to the\n",
            "particular tokenizer that it uses\n",
            "we are going to use the GPT 3.5 turbo\n",
            "model which uses the CL 100K base and I\n",
            "would say I think most of the more\n",
            "recent models like the models that you'd\n",
            "be using at the time of recording this\n",
            "video\n",
            "they they all use this encoder okay so\n",
            "the the embeddings model that is the\n",
            "most up-to-date uses cr100k base the you\n",
            "know track gpts uh GPT 2.5 turbo uses\n",
            "cr100k base GT4 also uses it the only\n",
            "one that is still kind of a relevant\n",
            "model is the text avengers003 model and\n",
            "that is the only relevant model that\n",
            "doesn't use that encoder so this one\n",
            "uses a p50k base all right so in reality\n",
            "you don't even need to go there to find\n",
            "out the encoding that you need to use\n",
            "you can actually just see this so take\n",
            "token encoding for model and you can you\n",
            "can run this right so you get the CL\n",
            "100K base that's how we know okay now\n",
            "anything else I think that is pretty\n",
            "much it so okay so actually here I'm\n",
            "creating this tick token length function\n",
            "so that is you're going to take some\n",
            "text it's going to use the tokenizer to\n",
            "calculate the length of that text in\n",
            "terms of tick token tokens\n",
            "that's important because we we need to\n",
            "use that for our line chain splitter\n",
            "function in a moment so we create that\n",
            "then what we can do is\n",
            "just first before we kind of jump into\n",
            "the whole chunking component I'm going\n",
            "to have a look at what the length of\n",
            "documents looks like at the moment so\n",
            "I'm going to calculate the token counts\n",
            "the tick token length function come to\n",
            "here we can see the minimum maximum and\n",
            "average number of tokens so the smallest\n",
            "document contains just 45 tokens this is\n",
            "probably I don't know this is probably a\n",
            "page that we don't really need it\n",
            "probably doesn't contain anything useful\n",
            "in that maximum is almost 58 000 tokens\n",
            "which is really big I'm not sure I'm not\n",
            "sure what that is but the average is a\n",
            "bit more normal so 1.3 000 there\n",
            "so we can kind of visualize the\n",
            "distribution of those of those pages and\n",
            "the map tokens they have so the vast\n",
            "majority of pages have a very like\n",
            "they're more towards the 1000 token\n",
            "range so we can sort of see it here all\n",
            "right cool now let's continue and we'll\n",
            "we'll start and look at how we're going\n",
            "to chunk everything so again we're using\n",
            "line chain here using a text splitter\n",
            "and we're using the recursive character\n",
            "text button now this is I think probably\n",
            "one of the best like chunkers or Tech\n",
            "Splitters that line chain offers at the\n",
            "moment it's very general purpose they do\n",
            "also offer some text Splitters that are\n",
            "more specific to like markdown for\n",
            "example but I you know I I like this one\n",
            "it you can use it for a ton of things so\n",
            "let me just explain it very quickly so\n",
            "basically what it's going to do is it's\n",
            "going to take your length function\n",
            "inside the tick token length and it's\n",
            "going to say I need to split your text\n",
            "so that each chunk does not go over this\n",
            "chunk size here so this 400 and it's\n",
            "going to split based on the separators\n",
            "okay so the reason we have multiple\n",
            "separate is is that it's first starts by\n",
            "trying to find double new lines so this\n",
            "is a double new line separator it's\n",
            "going to try and split on that first if\n",
            "it can't find a good split using the\n",
            "double new line\n",
            "characters it will just try a single new\n",
            "line then it will try space and as a\n",
            "very last result it will just split on\n",
            "anything okay okay cool and then one\n",
            "final thing that we have here is this\n",
            "chunk overlap so this chunk overlap is\n",
            "saying for every Chunk we are going to\n",
            "overlap with the next chunk by 20 tokens\n",
            "okay let me let me draw that out so it\n",
            "makes more sense okay so imagine we we\n",
            "have a ton of text okay\n",
            "there's loads of tapes here\n",
            "okay now we are going to get a chunk of\n",
            "is 400 characters right so let's say\n",
            "that chunk takes us from here all the\n",
            "way to say here okay so we have 400\n",
            "characters in this truck\n",
            "then the next chunk if we don't have any\n",
            "chunk overlap would be 400 characters\n",
            "from this so that would be you know\n",
            "let's say it's to here okay but this\n",
            "comes with a problem because we don't\n",
            "know what this information here and this\n",
            "information here is about so they could\n",
            "be related right so we might be missing\n",
            "out on some important information by\n",
            "just splitting in the middle here\n",
            "so it's important to try and avoid that\n",
            "if possible and the most\n",
            "naive way or naive approach for doing\n",
            "this is to include a trunk overlap so\n",
            "what we would do is let's say we take\n",
            "the 20 tokens behind this okay so we're\n",
            "gonna go back\n",
            "20 tokens which maybe comes to here okay\n",
            "so that means that this space here\n",
            "is now going to be shared by the last or\n",
            "the the first chunk and the next chunk\n",
            "which will also bring back the next\n",
            "chunk to something like here right so\n",
            "now we have\n",
            "chunk one here okay which goes from from\n",
            "here up to here\n",
            "and then we have chunk two which is\n",
            "from here\n",
            "to here\n",
            "then following on from that we would\n",
            "also add another like trunk overlap for\n",
            "number three so number three would go\n",
            "from here to let's say here and finally\n",
            "for number four we'll go from like here\n",
            "to here okay so the chunk overlap is\n",
            "just to make sure that we're not missing\n",
            "any important connections between our\n",
            "chunks okay it does mean that we're\n",
            "going to have a little bit more\n",
            "data to to store there okay because\n",
            "we're including like these chunks of 20\n",
            "in multiple places but I've I think\n",
            "that's usually worth it in terms of the\n",
            "better performance that you can get by\n",
            "not missing out that important\n",
            "information like important connection\n",
            "between chunks okay so we initialize\n",
            "that\n",
            "and then to actually split the text we\n",
            "use the text splitter split text okay\n",
            "we're going to take docs5 I'm going to\n",
            "take the page content okay which is just\n",
            "the plain text\n",
            "right\n",
            "So based on how the parameters that we\n",
            "set here Trump size of 400 and trunk\n",
            "overlap of 20 using the tick token lamp\n",
            "token we get two chunks let's have a\n",
            "look at the length of those two trunks\n",
            "okay so the first chunk that we get is\n",
            "346 tokens next one 247. so both within\n",
            "that Max upper end limit of 400 okay so\n",
            "you see that it's not going to\n",
            "necessarily split on the 400 tokens\n",
            "specifically because we have these\n",
            "specific separators that we would like\n",
            "to use okay and it's going to optimize\n",
            "preferably for this separator\n",
            "okay so we're not going right up to that\n",
            "limit with every single chunk uh which\n",
            "is is fine that's kind of Ideal we don't\n",
            "want to we don't we don't necessarily\n",
            "need to put in a ton of text there\n",
            "okay so that's it for a single document\n",
            "and what we're going to do now is we're\n",
            "going to repeat that over the entire\n",
            "data set and the final format that I\n",
            "want to create here is going to look\n",
            "like this okay so we're going to have\n",
            "the ID we're going to have our text I'm\n",
            "going to have the source where this text\n",
            "is actually come from okay\n",
            "now one thing that you'll notice here is\n",
            "the ID okay so we're going to create an\n",
            "ID and that ID will be unique to each\n",
            "page okay but we're going to have\n",
            "multiple chunks for each page so that\n",
            "means we're also going to add in this\n",
            "like chunk identifier onto the end of\n",
            "the ID to make sure that every ID for\n",
            "every chunk is actually unique\n",
            "so let's let me show you how we're going\n",
            "to create that essentially so we have\n",
            "the URL here\n",
            "okay we're going to replace the RT dots\n",
            "that we have here with the actual https\n",
            "protocol and I'm just going to print out\n",
            "so you can see what it is and then we're\n",
            "going to take that URL\n",
            "we're going to add it to this hashlib\n",
            "md5 so this is just a hashing function\n",
            "that is going to take our URL and hash\n",
            "it into kind of like a unique identifier\n",
            "right\n",
            "so this is useful because if we are\n",
            "updating this text at some point in the\n",
            "future or this data set sorry we can use\n",
            "the same hashing function to create our\n",
            "unique IDs and that means that when we\n",
            "update this particular page it will just\n",
            "overwrite the previous\n",
            "versions of that item right because\n",
            "we're using the same ID but of course we\n",
            "don't we can't use the same ID for every\n",
            "single chunk so we also need to add in\n",
            "this here which is like the the chunk\n",
            "identifier right it's just it's just a\n",
            "count of the number of chunks so we can\n",
            "see that being created here so these are\n",
            "just two examples from the previous page\n",
            "that we we just showed so you can see we\n",
            "have the chunk identifier and indeed the\n",
            "chunks are different so this says\n",
            "language model Cascades ice primary\n",
            "books Socratic models okay whatever\n",
            "let's take a look at what is at the end\n",
            "of the item and it should be something\n",
            "similar so there should be the overlap\n",
            "that I mentioned right\n",
            "okay so yeah you can see language model\n",
            "Cascades ice Prime boat Socratic models\n",
            "right same thing cool\n",
            "so there is the overlap right now what\n",
            "we need to do is repeat this same logic\n",
            "that we've just created across our\n",
            "entire data set so to do that same thing\n",
            "that we just did we're going to take the\n",
            "URL out we're going to create our unique\n",
            "ID we're going to take the chunks using\n",
            "the text splitter and then we're going\n",
            "to append these all to our documents\n",
            "list here okay that's just going to be\n",
            "where we store everything okay and now\n",
            "so let the documents an hour ago was was\n",
            "a little bit less now it is\n",
            "2012 documents so\n",
            "sorry\n",
            "2212 documents\n",
            "cool we cannot save them to Jason lines\n",
            "file at you that we we just do this so\n",
            "Json lines it's basically it's what you\n",
            "can see here right so if we take a look\n",
            "at documents look the first five it's\n",
            "this but it's just in a Json lines file\n",
            "okay so you can see it here yeah same\n",
            "thing right okay and then once you've\n",
            "saved it and you've created your jsonnl\n",
            "file you would just load it from file\n",
            "like this okay so you with open Train\n",
            "Jason now wherever you saw it and you\n",
            "just load it iteratively like that\n",
            "okay you can take a look\n",
            "yeah okay great so that's how you would\n",
            "load it now a couple of things here\n",
            "the reason that we're using Json L and\n",
            "the reason I'm calling this train.jsonl\n",
            "is because this makes it very compatible\n",
            "with hugging face data sets which is\n",
            "essentially a way of sharing your data\n",
            "set with others or just making it more\n",
            "accessible for yourself if you set to\n",
            "being a private data set so what I want\n",
            "to do is just show you how we can\n",
            "actually go about doing that as well so\n",
            "the first thing that we need to do is go\n",
            "to hookingface.co and that will bring\n",
            "you to the the first page of Hogan\n",
            "features it may look different to you\n",
            "and because you you may not already have\n",
            "an account on active face so if you do\n",
            "need an account or you need to sign in\n",
            "there will be a little button over here\n",
            "that says sign up or log in so you would\n",
            "follow that create your account or log\n",
            "in and then you will see something like\n",
            "this at which point you go over to your\n",
            "profile\n",
            "click new data set we give our data set\n",
            "a name I'm going to call it langchain\n",
            "dots you can obviously call this\n",
            "whatever you want you can set it private\n",
            "if you want to keep this data set\n",
            "private for me also I'm going to just\n",
            "leave it as public and you create your\n",
            "data set right so on here this is like\n",
            "the the page of your data set like the\n",
            "home page of your data set you go to\n",
            "files you go to add file upload files\n",
            "okay and then you just need to drag in\n",
            "the train dot Json L file to here so for\n",
            "me that is here I'm just going to go and\n",
            "drag that in okay we go down commit\n",
            "changes to main okay so we have now\n",
            "uploaded that we can go click on files\n",
            "here and we'll be able to see that we\n",
            "have the train.jsonl file in there now\n",
            "to actually use that in our code we\n",
            "would need to install data set so this\n",
            "is a like the library for hugging face\n",
            "data sets and then we would write this\n",
            "so do firm data sets\n",
            "import load data set\n",
            "and then our our data would be a load\n",
            "data set\n",
            "here we need the name of our data set so\n",
            "let's go back to the to the data set\n",
            "page okay we can find that at the top\n",
            "here so it's James Kellum line chain\n",
            "dots we can just copy it add that into\n",
            "here\n",
            "uh split\n",
            "is the training split\n",
            "so that's where the train.json hour\n",
            "comes in and then we can view the data\n",
            "details there\n",
            "okay and once that has loaded we will be\n",
            "able to see we can just kind of extract\n",
            "things so\n",
            "dates zero we can see that we have\n",
            "our text in there so it's super easy to\n",
            "work with and that's kind of like why I\n",
            "recommend storing your data on hook\n",
            "image data sets\n",
            "if you're wanting to share it and even\n",
            "if you you're wanting to do the private\n",
            "approach you can you can do that as well\n",
            "you just need I think it's like an API\n",
            "key and that's pretty much it so that's\n",
            "it for this video I just wanted to cover\n",
            "some of the\n",
            "approaches that we take when we are\n",
            "considering how to chunk our text and\n",
            "actually process it for large language\n",
            "models and also see how we might saw\n",
            "that data later on as well\n",
            "which you know both of these items I\n",
            "think we kind of miss a lot in the\n",
            "typical videos about really focusing on\n",
            "the large language model processing or\n",
            "the retrieval augmentation or whatever\n",
            "else right so this in reality is\n",
            "probably one of the most important parts\n",
            "of the entire process but we miss it\n",
            "Miss it pretty often anyway that's it\n",
            "for this video so thank you very much\n",
            "for watching I hope this has all been\n",
            "useful and interesting and I will see\n",
            "you again in the next one bye\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "If you learned something, consider following me at [https://twitter.com/aigen__](https://twitter.com/aigen__)"
      ],
      "metadata": {
        "id": "yLcQwC5Fx4La"
      }
    }
  ]
}